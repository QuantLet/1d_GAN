{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "from skimage.io import imread_collection\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## directories\n",
    "paths = ['images',\n",
    "            'images/1dim_uni_to_stnormal',\n",
    "                'images/1dim_uni_to_stnormal/gif',\n",
    "                'images/1dim_uni_to_stnormal/final',\n",
    "         'images/1dim_norm(0,8)_to_stnormal',\n",
    "         'images/1dim_norm(8,1)_to_stnormal',\n",
    "                'images/1dim_norm(8,1)_to_stnormal/gif',\n",
    "                'images/1dim_norm(8,1)_to_stnormal/final',\n",
    "        ]\n",
    "\n",
    "for i in paths:\n",
    "    if not os.path.exists(i):\n",
    "        os.makedirs(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(samples, dimensions=2):\n",
    "    return np.random.uniform(-1,1, (samples, dimensions))\n",
    "    #return np.random.normal(0,8, (samples,dimensions))\n",
    "    #return np.random.normal(8,1, (samples,dimensions))\n",
    "    \n",
    "def generate_data(samples, dimensions=2):\n",
    "    return np.random.normal(0,1, (samples, dimensions))\n",
    "\n",
    "# mapping (R,R)-->([-1,1], [-1,1])\n",
    "def generator(latent_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, activation = 'relu', kernel_initializer= 'he_uniform', input_dim = latent_dim))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(Dense(16, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(Dense(16, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(Dense(16, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(Dense(output_dim, activation = 'linear'))\n",
    "    return model\n",
    "\n",
    "def discriminator(dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation = 'relu', kernel_initializer = 'he_uniform', input_dim = dim))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(Dense(64, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(Dense(64, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
    "    model.add(LeakyReLU(alpha = 0.1))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(optimizer = Adam(learning_rate = 0.002, beta_1 = 0.5), metrics = ['accuracy'], loss = 'binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# build GAN given a discriminator and a generator\n",
    "def GAN(G, D):\n",
    "    D.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(G)\n",
    "    model.add(D)\n",
    "    model.compile(optimizer = Adam(learning_rate = 0.001, beta_1 = 0.5), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "\n",
    "def plots(G, D, step, D_loss, G_loss, filename):\n",
    "    file, ax = plt.subplots(2, 2, figsize=(8,8))\n",
    "    \n",
    "    # plot losses\n",
    "    ax[0,0].plot(step, G_loss, label='G loss', \n",
    "                    c='darkgreen', zorder=50, alpha=0.8,)\n",
    "    ax[0,0].plot(step, D_loss, label='D loss',\n",
    "                    c='darkblue', zorder=55, alpha=0.8,)\n",
    "    ax[0,0].set_xlim(0, max(max(step), batches)+5)\n",
    "    ax[0,0].set_ylim(0.45, 1.0)\n",
    "    ax[0,0].set_xlabel('Epoch')\n",
    "    ax[0,0].legend(loc=1, frameon=False)\n",
    "    \n",
    "    # plot real and generated samples\n",
    "    fake_samples = G.predict(test_noise, batch_size=len(test_noise))\n",
    "    x_val = np.linspace(-3, 3, 301)\n",
    "    y_val = stats.norm(0,1).pdf(x_val)\n",
    "    ax[0,1].plot(x_val, y_val, color='k', label='real')\n",
    "    ax[0,1].fill_between(x_val, np.zeros(len(x_val)), y_val, color='k', alpha=0.6)\n",
    "    sns.kdeplot(fake_samples.flatten(), color='green', alpha=0.6, label='GAN', ax=ax[0,1], shade=True)\n",
    "    #ax[0,1].set_xlim(-3,3)\n",
    "    ax[0,1].legend(loc=1, frameon=False)\n",
    "    ax[0,1].set_xlabel('Sample Space')\n",
    "    ax[0,1].set_ylabel('Density')\n",
    "    \n",
    "    # plot confidence of the discriminator\n",
    "    confi = D.predict(grid_sample, batch_size=size).flatten()\n",
    "    ax[1,0].plot(grid_sample.flatten(), confi, c='b')\n",
    "    lower, upper = -3, 3\n",
    "    for i in range(0, len(confi), 50):\n",
    "        if i==0:\n",
    "            continue\n",
    "        ax[1,0].plot([(i/len(confi))*(upper-lower) + lower, ]*2,\n",
    "                        [0, confi[i]], c='b')\n",
    "    ax[1,0].fill_between(grid_sample.flatten(), np.zeros(len(confi)), confi, color='b', alpha=0.6)\n",
    "    ax[1,0].set_xlabel('Sample Space')\n",
    "    ax[1,0].set_ylabel('Discriminator Value')\n",
    "    ax[1,0].set_xlim(lower,upper)\n",
    "    ax[1,0].set_ylim(0.0,1.0)\n",
    "        \n",
    "    # Q-Q Plot\n",
    "    qq = fake_samples.reshape(len(fake_samples,))\n",
    "    sm.qqplot(qq, line='45', markerfacecolor='mediumblue', markeredgecolor='mediumblue',  ax=ax[1,1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, transparent=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "# latent dimension\n",
    "latent_dim = 1\n",
    "\n",
    "# subsample for gif, set to zero if no gif wanted\n",
    "batches = 500\n",
    "\n",
    "# batch size\n",
    "size = 512\n",
    "\n",
    "# plotting frequency for Ggif\n",
    "freq = 1\n",
    "\n",
    "# samples for final plot\n",
    "test_noise = generate_noise(5000, latent_dim)\n",
    "test_samples = generate_data(5000, latent_dim)\n",
    "\n",
    "G = generator(latent_dim, 1)\n",
    "D = discriminator(1)\n",
    "GAN = GAN(G, D)\n",
    "\n",
    "grid_sample = np.linspace(-3, 3, 603)[1:-1].reshape((-1, 1))\n",
    "\n",
    "step = []\n",
    "D_acc = []\n",
    "G_acc = []\n",
    "D_loss = []\n",
    "G_loss = []\n",
    "count = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 500/500\n"
     ]
    }
   ],
   "source": [
    "# produce pictures for GIF\n",
    "for k in range(batches):\n",
    "    print(f'step: {k+1}/{batches}', end='\\r')\n",
    "    \n",
    "    # train discriminator\n",
    "    D.trainable = True\n",
    "    real_data = generate_data(size//2,latent_dim)\n",
    "    fake_data = G.predict(generate_noise(size//2, latent_dim), batch_size=size//2)\n",
    "    data = np.concatenate((real_data, fake_data), axis=0)\n",
    "    \n",
    "    real_labels = np.ones((size//2, 1))\n",
    "    fake_labels = np.zeros((size//2,1))\n",
    "    labels = np.concatenate((real_labels, fake_labels), axis=0)\n",
    "    \n",
    "    _D_loss, _D_acc = D.train_on_batch(data, labels)\n",
    "    \n",
    "    # train generator\n",
    "    D.trainable = False\n",
    "    noise = generate_noise(size, latent_dim)\n",
    "    labels = np.ones((size, 1))\n",
    "    _G_loss, _G_acc = GAN.train_on_batch(noise, labels)\n",
    "    \n",
    "    if k % freq == 0:\n",
    "        step.append(k)\n",
    "        D_loss.append(_D_loss)\n",
    "        G_loss.append(_G_loss)\n",
    "        plots(G=G, D=D, step=step, D_loss=D_loss, G_loss=G_loss,\n",
    "                filename=f'images/1dim_uni_to_stnormal/gif/pic.{k+1:03d}.jpeg')\n",
    "                #filename=f'images/1dim_norm(8,1)_to_stnormal/gif/pic.{k+1:03d}.jpeg')\n",
    "        plt.close()\n",
    "print()\n",
    "\n",
    "#make GIF\n",
    "directory = 'images/1dim_uni_to_stnormal/gif/*.jpeg'\n",
    "#directory = 'images/1dim_norm(8,1)_to_stnormal/gif/*.jpeg'\n",
    "images = imread_collection(directory)\n",
    "imageio.mimsave('images/1dim_uni_to_stnormal/final/animation.mp4', images, quality=9)\n",
    "#imageio.mimsave('images/1dim_norm(8,1)_to_stnormal/final/animation.mp4', images, quality=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9870/10000\r"
     ]
    }
   ],
   "source": [
    "total_steps = 10000\n",
    "\n",
    "for k in range(total_steps):\n",
    "    print(f'step: {k+1}/{total_steps}', end='\\r')\n",
    "    \n",
    "    # train discriminator\n",
    "    D.trainable = True\n",
    "    real_data = generate_data(size//2,latent_dim)\n",
    "    fake_data = G.predict(generate_noise(size//2, latent_dim), batch_size=size//2)\n",
    "    data = np.concatenate((real_data, fake_data), axis=0)\n",
    "    \n",
    "    real_labels = np.ones((size//2, 1))\n",
    "    fake_labels = np.zeros((size//2,1))\n",
    "    labels = np.concatenate((real_labels, fake_labels), axis=0)\n",
    "    \n",
    "    _D_loss, _D_acc = D.train_on_batch(data, labels)\n",
    "    \n",
    "    # train generator\n",
    "    D.trainable = False\n",
    "    noise = generate_noise(size, latent_dim)\n",
    "    labels = np.ones((size, 1))\n",
    "    _G_loss, _G_acc = GAN.train_on_batch(noise, labels)    \n",
    "    \n",
    "    if k % freq == 0:\n",
    "        step.append(k)\n",
    "        D_loss.append(_D_loss)\n",
    "        G_loss.append(_G_loss)\n",
    "        \n",
    "plots(G=G, D=D, step=step, D_loss=D_loss, G_loss=G_loss, filename=f'images/1dim_uni_to_stnormal/final/plot_step{k+1:03d}.png')\n",
    "#plots(G=G, D=D, step=step, D_loss=D_loss, G_loss=G_loss, filename=f'images/1dim_norm(0,8)_to_stnormal/plot_step{k+1:03d}.png')\n",
    "#plots(G=G, D=D, step=step, D_loss=D_loss, G_loss=G_loss, filename=f'images/1dim_norm(8,1)_to_stnormal/plot_step{k+1:03d}_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
